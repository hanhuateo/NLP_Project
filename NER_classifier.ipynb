{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir('/content/drive/MyDrive/Y4S1/CZ4045 Natural Language Processing/Project/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRy6mytpP5Bk","executionInfo":{"status":"ok","timestamp":1699541865483,"user_tz":-480,"elapsed":19851,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"b1cb99e5-a0e2-46d9-c7ba-01c165f9af37"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# Part 1.1 Word Embedding\n","Based on word2vec embeddings you have downloaded, use cosine similarity to find the most similar word to each of these words: (a) “student”; (b) “Apple”; (c) “apple”. Report the most similar word and its cosine similarity.\n"],"metadata":{"id":"hSpxiYDxPuF1"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"-sKg2aFVPqe_","executionInfo":{"status":"ok","timestamp":1699541875771,"user_tz":-480,"elapsed":1227,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}}},"outputs":[],"source":["import gensim.downloader as api"]},{"cell_type":"code","source":["from gensim.models import KeyedVectors\n","model = KeyedVectors.load('./data/word2vec_vectors.kv')"],"metadata":{"id":"ik-j7PXMZQwC","executionInfo":{"status":"ok","timestamp":1699541916901,"user_tz":-480,"elapsed":41133,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(\"The most similar word to student is :\",(model.most_similar(\"student\", topn=1))[0][0], \"\\nand its cosine similarity is:\", (model.most_similar(\"student\", topn=1))[0][1])\n","print(\"\\nThe most similar word to Apple is:\",(model.most_similar(\"Apple\", topn=1))[0][0], \"\\nand its cosine similarity is:\", (model.most_similar(\"Apple\", topn=1))[0][1])\n","print(\"\\nThe most similar word to apple is:\",(model.most_similar(\"apple\", topn=1))[0][0], \"\\nand its cosine similarity is:\", (model.most_similar(\"apple\", topn=1))[0][1])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uQyUNQNCZiWp","executionInfo":{"status":"ok","timestamp":1699541918924,"user_tz":-480,"elapsed":2026,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"55d613cf-982f-462e-d230-7895c2b18192"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The most similar word to student is : students \n","and its cosine similarity is: 0.7294867038726807\n","\n","The most similar word to Apple is: Apple_AAPL \n","and its cosine similarity is: 0.7456986308097839\n","\n","The most similar word to apple is: apples \n","and its cosine similarity is: 0.720359742641449\n"]}]},{"cell_type":"markdown","source":["# Part 1.2 Data"],"metadata":{"id":"z4uDAM80PyT2"}},{"cell_type":"code","source":["def preprocess_data(file_path):\n","    with open(file_path, \"r\") as file:\n","        lines = file.readlines()\n","\n","    sentences = []\n","    sentence_words = []\n","    sentence_labels = []\n","    IO_labels_list = []\n","\n","    for line in lines:\n","        tokens = line.strip().split()\n","        if tokens: # if the line is not empty\n","            word = tokens[0]\n","            if word == '-DOCSTART-':\n","                continue\n","            label = tokens[-1]\n","            if 'B-' in label:\n","                label = label.replace('B-', 'I-', 1)\n","            if label not in IO_labels_list:\n","                IO_labels_list.append(label)\n","            sentence_words.append(word)\n","            sentence_labels.append(label)\n","        else:\n","            if sentence_words and sentence_labels:\n","                sentences.append((list(sentence_words), list(sentence_labels)))\n","                sentence_words.clear()\n","                sentence_labels.clear()\n","\n","    print(f\"The tagging labels list for {file_path[7:]} are : {IO_labels_list}\")\n","    return sentences\n","\n","train_sentences = preprocess_data('./data/eng.train')\n","development_sentences = preprocess_data('./data/eng.testa')\n","test_sentences = preprocess_data('./data/eng.testb')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxS_XVxVeKzE","executionInfo":{"status":"ok","timestamp":1699541921224,"user_tz":-480,"elapsed":2303,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"5abf3b72-ebae-403d-8079-b474ad897ef2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["The tagging labels list for eng.train are : ['I-ORG', 'O', 'I-MISC', 'I-PER', 'I-LOC']\n","The tagging labels list for eng.testa are : ['O', 'I-ORG', 'I-LOC', 'I-MISC', 'I-PER']\n","The tagging labels list for eng.testb are : ['O', 'I-LOC', 'I-PER', 'I-MISC', 'I-ORG']\n"]}]},{"cell_type":"markdown","source":["(a) Describe the size (number of sentences) of the training, development and test file for CoNLL2003. Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO, etc.) you chose."],"metadata":{"id":"qKl_xs47I3nh"}},{"cell_type":"code","source":["print(f\"Number of sentences for train data is : {len(train_sentences)}\")\n","print(f\"Number of sentences for development data is : {len(development_sentences)}\")\n","print(f\"Number of sentences for test data is : {len(test_sentences)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4tQdd4fJBOZ","executionInfo":{"status":"ok","timestamp":1699541921225,"user_tz":-480,"elapsed":4,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"8322e8fe-f678-482d-920a-c4e09fedc9a3"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of sentences for train data is : 14041\n","Number of sentences for development data is : 3250\n","Number of sentences for test data is : 3453\n"]}]},{"cell_type":"markdown","source":["For the tagging scheme, we have decided to choose the IO scheme.\n","The complete set of all possible word labels are:\n","1. I-PER\n","2. I-ORG\n","3. I-MISC\n","4. I-LOC\n","8. O"],"metadata":{"id":"sJrJ-r2TJH1I"}},{"cell_type":"markdown","source":["(b) Choose an example sentence from the training set of CoNLL2003 that has at least two named entities with more than one word. Explain how to form complete named entities from the label for each word, and list all the named entities in this sentence."],"metadata":{"id":"NQv8uyr0eWQY"}},{"cell_type":"code","source":["import random\n","def has_multi_word_entities(sentence, labels):\n","    current_entity = []\n","    multi_word_entities_count = 0\n","\n","    for word, label in zip(sentence, labels):\n","        if label.startswith('I-'):\n","            current_entity.append(word)\n","        elif current_entity:\n","            if len(current_entity) > 1:\n","                multi_word_entities_count += 1\n","            current_entity = []\n","\n","    if current_entity and len(current_entity) > 1:\n","        multi_word_entities_count += 1\n","\n","    return multi_word_entities_count >= 2\n","\n","def sentences_with_criteria(dataset):\n","    result = []\n","\n","    for sentence, labels in dataset:\n","        if has_multi_word_entities(sentence, labels):\n","            result.append((sentence, labels))\n","\n","    return result\n","\n","# Extract sentences and labels that meet the criteria\n","filtered_train_sentences = sentences_with_criteria(train_sentences)\n","\n","random_sentence_idx =  random.randint(0, len(filtered_train_sentences)-1)\n","\n","\n","print(\"The sentence choosen is: \")\n","for word in filtered_train_sentences[random_sentence_idx][0]:\n","    print(word, end=\" \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZWB0sMVMKhkr","executionInfo":{"status":"ok","timestamp":1699541928713,"user_tz":-480,"elapsed":5,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"23509c29-7649-4ae0-e6dc-8c570f371be4"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["The sentence choosen is: \n","\" I do n't normally do this but can you please sign , \" he said thrusting an ornate white book in front of Americans Harrison Dillard ( 1948 ) , Lindy Remigino ( 1952 ) , Jim Hines ( 1968 ) , Trinidad 's Hasely Crawford ( 1976 ) and Britain 's Allan Wells ( 1980 ) . "]}]},{"cell_type":"markdown","source":["**In order to form complete named entities from the label for each word**\n","\n","1. We start with the first word in the sequence.\n","2. If we encounter a word with an I- label, it's part of a named entity. Collect the words with consecutive I- labels of the same type to form multi-word entities.\n","4. If we encounter a word with an O label, or an I- label, or an I- label that is of a different type (e.g. 'I-MISC', 'I-PER') , it indicates the end of the current entity and possibly the start of a new one.\n","5. We continue this process until we've traversed the entire sequence."],"metadata":{"id":"1jay_BlcTYLD"}},{"cell_type":"code","source":["def extract_entities_io(words, labels):\n","\n","    entities = []\n","    current_entity_words = []\n","    current_entity_type = None\n","\n","    for word, label in zip(words, labels):\n","        if label.startswith(\"I-\"):\n","            label_type = label.split(\"-\")[1]\n","\n","            # If we have a current entity and the label type has changed, or an unexpected label is encountered\n","            if current_entity_words and (not current_entity_type or current_entity_type != label_type):\n","                entities.append((current_entity_type, \" \".join(current_entity_words)))\n","                current_entity_words = []\n","\n","            # Update the current entity type and add the word to the current entity\n","            current_entity_type = label_type\n","            current_entity_words.append(word)\n","\n","        else:  # For 'O' labels or any other unexpected label\n","            if current_entity_words:\n","                entities.append((current_entity_type, \" \".join(current_entity_words)))\n","                current_entity_words = []\n","                current_entity_type = None\n","\n","    # Handle any remaining entity at the end of the sequence\n","    if current_entity_words:\n","        entities.append((current_entity_type, \" \".join(current_entity_words)))\n","\n","    return entities\n","\n","named_entities = extract_entities_io(filtered_train_sentences[random_sentence_idx][0],filtered_train_sentences[random_sentence_idx][1])\n","print(\"Named entities of this sentence are: \")\n","for idx, entities in enumerate(named_entities, start=1):\n","    print(str(idx) + \". \" + entities[1] + \", TAG = \" + entities[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyRzMvbPV0IF","executionInfo":{"status":"ok","timestamp":1699541939408,"user_tz":-480,"elapsed":360,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"08fb5a1c-d848-4cb5-8391-16713ffb4a4b"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Named entities of this sentence are: \n","1. Americans, TAG = MISC\n","2. Harrison Dillard, TAG = PER\n","3. Lindy Remigino, TAG = PER\n","4. Jim Hines, TAG = PER\n","5. Trinidad, TAG = LOC\n","6. Hasely Crawford, TAG = PER\n","7. Britain, TAG = LOC\n","8. Allan Wells, TAG = PER\n"]}]},{"cell_type":"markdown","source":["# Part 1.3 Model"],"metadata":{"id":"_6cWElzIivDY"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","torch.manual_seed(1)\n","import numpy as np"],"metadata":{"id":"0z6Uw39xA_wt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_data(train_sentences):\n","    data = []\n","\n","    for sentence, tags in train_sentences:\n","        for word, tag in zip(sentence, tags):\n","            data.append((word, tag))\n","\n","    return data\n","\n","train_data = prepare_data(train_sentences)\n","development_data = prepare_data(development_sentences)\n","test_data = prepare_data(test_sentences)\n","print(train_sentences[0])\n","print(train_data[0:9])"],"metadata":{"id":"-qP2YOjFD1ts","executionInfo":{"status":"ok","timestamp":1698825351161,"user_tz":-480,"elapsed":4,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2759fb8e-c7aa-414b-d86b-83666312596c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['I-ORG', 'O', 'I-MISC', 'O', 'O', 'O', 'I-MISC', 'O', 'O'])\n","[('EU', 'I-ORG'), ('rejects', 'O'), ('German', 'I-MISC'), ('call', 'O'), ('to', 'O'), ('boycott', 'O'), ('British', 'I-MISC'), ('lamb', 'O'), ('.', 'O')]\n"]}]},{"cell_type":"code","source":["word_vectors = KeyedVectors.load(\"./data/word2vec_vectors.kv\", mmap='r')"],"metadata":{"id":"F6w4ogxKD81I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create word and tag mappings\n","words = list(set(word for word, tag in train_data))\n","tags = list(set(tag for word, tag in train_data))\n","\n","# Creating the mappings\n","word2idx = {word: i + 1 for i, word in enumerate(words)}  # +1 to leave 0 for padding\n","tag2idx = {tag: i for i, tag in enumerate(tags)}\n","idx2tag = {i: tag for tag, i in tag2idx.items()}"],"metadata":{"id":"iTSKIwS9ND_q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# print(idx2tag)"],"metadata":{"id":"z5EwNz_uuocA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create embedding matrix\n","embedding_dim = word_vectors.vector_size\n","embedding_matrix = np.zeros((len(word2idx) + 1, embedding_dim))  # +1 for padding token\n","\n","for word, i in word2idx.items():\n","    try:\n","        embedding_matrix[i] = word_vectors[word]\n","    except KeyError:\n","        # Word not in pretrained embeddings; initialize randomly\n","        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim, ))\n","\n","# Convert data to integer sequences\n","def encode_sentences_flattened(data, word2idx, tag2idx):\n","    X_data = [word2idx.get(word, 0) for word, tag in data]  # 0 for OOV words\n","    Y_data = [tag2idx[tag] for word, tag in data]\n","    return X_data, Y_data\n","\n","X_train, Y_train = encode_sentences_flattened(train_data, word2idx, tag2idx)\n","X_dev, Y_dev = encode_sentences_flattened(development_data, word2idx, tag2idx)\n","X_test, Y_test = encode_sentences_flattened(test_data, word2idx, tag2idx)\n","\n","X_train[:5], Y_train[:5]  # Displaying the first 5 encoded words and their tags for verification"],"metadata":{"id":"jykVN1liNG8u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698825353457,"user_tz":-480,"elapsed":979,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"1f28d0cb-4190-41e5-e25b-c90f34b91d45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["([6947, 12257, 22426, 9997, 15139], [4, 0, 2, 0, 0])"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class NERDataset(Dataset):\n","    def __init__(self, X, Y):\n","        self.X = X\n","        self.Y = Y\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, index):\n","        return self.X[index], self.Y[index]\n","\n","# Create instances of the custom dataset\n","train_dataset = NERDataset(X_train, Y_train)\n","dev_dataset = NERDataset(X_dev, Y_dev)\n","test_dataset = NERDataset(X_test, Y_test)\n","# Create data loaders\n","BATCH_SIZE = 128  # Define your desired batch size\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"],"metadata":{"id":"0B6BZiCa3tCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NERModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix, dropout_rate = 0.5):\n","        super(NERModel, self).__init__()\n","\n","        # Embedding layer with pretrained embeddings\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n","        self.embedding.weight.requires_grad = False  # Freeze the pretrained embeddings\n","\n","        # Bidirectional LSTM layer\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n","\n","        # Dropout layer\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        # Dense layer\n","        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)  # times 2 because of bidirectionality\n","\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        lstm_out, _ = self.lstm(embedded)\n","        # Apply dropout and then pass through fc to get refined representation\n","        dropped = self.dropout(lstm_out)\n","        dense_out = self.fc(dropped)\n","\n","        # Pass through fc2 to get final tag predictions\n","        tag_space = self.fc2(dense_out)\n","        return tag_space\n","\n","# Hyperparameters\n","EMBEDDING_DIM = embedding_matrix.shape[1]\n","HIDDEN_DIM = 256\n","OUTPUT_DIM = len(tag2idx)\n","\n","# Create an instance of the NER model\n","model = NERModel(len(word2idx) + 1, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, embedding_matrix)\n","\n","# Loss function and optimizer\n","learning_rate = 0.001\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"t5K5lwr98-h8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n","# Y_train_tensor = torch.tensor(Y_train, dtype=torch.long)\n","\n","# epochs = 50\n","# for epoch in range(epochs):\n","#     model.train()\n","\n","#     optimizer.zero_grad()\n","\n","#     predictions = model(X_train_tensor)\n","#     loss = criterion(predictions.view(-1, OUTPUT_DIM), Y_train_tensor.view(-1))\n","\n","#     loss.backward()\n","#     optimizer.step()\n","\n","#     print(f\"Epoch: {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n","\n","# ------------------------------------------------------------------------------\n","# epochs = 50\n","# for epoch in range(epochs):\n","#     model.train()\n","#     total_loss = 0\n","\n","#     for batch_X, batch_Y in train_loader:\n","#         optimizer.zero_grad()\n","#         predictions = model(batch_X)\n","#         loss = criterion(predictions.view(-1, OUTPUT_DIM), batch_Y.view(-1))\n","#         loss.backward()\n","#         optimizer.step()\n","#         total_loss += loss.item()\n","\n","#     avg_loss = total_loss / len(train_loader)\n","#     print(f\"Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","# ------------------------------------------------------------------------------\n","from sklearn.metrics import f1_score\n","import time\n","def compute_f1(model, loader):\n","    model.eval()\n","    all_preds = []\n","    all_true = []\n","\n","    with torch.no_grad():\n","        for batch_X, batch_Y in loader:\n","            predictions = model(batch_X)\n","            _, predicted_tags = torch.max(predictions, 1)  # get the index of the max probability\n","            all_preds.extend(predicted_tags.view(-1).cpu().numpy())\n","            all_true.extend(batch_Y.view(-1).cpu().numpy())\n","\n","    return f1_score(all_true, all_preds, average='macro')  # you can use 'micro' or 'weighted' based on your preference\n","\n","def evaluate_model(model, data_loader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch_X, batch_Y in data_loader:\n","            predictions = model(batch_X)\n","            loss = criterion(predictions.view(-1, OUTPUT_DIM), batch_Y.view(-1))\n","            total_loss += loss.item()\n","    return total_loss / len(data_loader)\n","\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","# Initialize the learning rate scheduler (adjusts learning rate based on validation loss)\n","scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n","\n","epochs = 100  # Increased as early stopping might stop it earlier\n","best_val_loss = float('inf')\n","epochs_without_improvement = 0\n","MAX_EPOCHS_WITHOUT_IMPROVEMENT = 10  # Number of epochs to wait before stopping\n","best_val_f1 = 0\n","start_time = time.time()\n","for epoch in range(epochs):\n","    model.train()\n","    total_loss = 0\n","\n","    for batch_X, batch_Y in train_loader:\n","        optimizer.zero_grad()\n","        predictions = model(batch_X)\n","        loss = criterion(predictions.view(-1, OUTPUT_DIM), batch_Y.view(-1))\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","\n","    # Evaluate on validation set\n","    val_loss = evaluate_model(model, dev_loader, criterion)\n","    val_f1 = compute_f1(model, dev_loader)\n","\n","    print(f\"Epoch: {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation F1: {val_f1:.4f}\")\n","\n","\n","    # Save the model with the best validation loss\n","    if val_f1 > best_val_f1:\n","        best_val_f1 = val_f1\n","        torch.save(model.state_dict(), 'best_model.pt')\n","        epochs_without_improvement = 0  # reset the count\n","    else:\n","        epochs_without_improvement += 1\n","\n","    # Adjust learning rate based on validation f1\n","    scheduler.step(val_f1)\n","\n","    # Early stopping\n","    if epochs_without_improvement >= MAX_EPOCHS_WITHOUT_IMPROVEMENT:\n","        print(\"Early stopping due to no improvement in validation f1.\")\n","        break\n","end_time = time.time()\n","run_time = end_time - start_time\n","print(f\"run time is : {run_time:.2f} seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcgBSjSX9Xgo","executionInfo":{"status":"ok","timestamp":1698830249143,"user_tz":-480,"elapsed":794393,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"61b9519e-bde9-431b-f6fc-e2265a7f80bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/100, Train Loss: 0.2391, Validation Loss: 0.2434, Validation F1: 0.7645\n","Epoch: 2/100, Train Loss: 0.1695, Validation Loss: 0.2256, Validation F1: 0.7834\n","Epoch: 3/100, Train Loss: 0.1409, Validation Loss: 0.2134, Validation F1: 0.7863\n","Epoch: 4/100, Train Loss: 0.1221, Validation Loss: 0.1963, Validation F1: 0.8016\n","Epoch: 5/100, Train Loss: 0.1104, Validation Loss: 0.1908, Validation F1: 0.8017\n","Epoch: 6/100, Train Loss: 0.1043, Validation Loss: 0.1901, Validation F1: 0.8038\n","Epoch: 7/100, Train Loss: 0.0987, Validation Loss: 0.1828, Validation F1: 0.8075\n","Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n","Epoch: 8/100, Train Loss: 0.0897, Validation Loss: 0.1825, Validation F1: 0.8112\n","Epoch: 9/100, Train Loss: 0.0867, Validation Loss: 0.1769, Validation F1: 0.8098\n","Epoch: 10/100, Train Loss: 0.0844, Validation Loss: 0.1759, Validation F1: 0.8109\n","Epoch: 11/100, Train Loss: 0.0822, Validation Loss: 0.1780, Validation F1: 0.8112\n","Epoch: 12/100, Train Loss: 0.0802, Validation Loss: 0.1748, Validation F1: 0.8115\n","Epoch: 13/100, Train Loss: 0.0792, Validation Loss: 0.1777, Validation F1: 0.8132\n","Epoch 00013: reducing learning rate of group 0 to 2.5000e-04.\n","Epoch: 14/100, Train Loss: 0.0759, Validation Loss: 0.1741, Validation F1: 0.8148\n","Epoch: 15/100, Train Loss: 0.0743, Validation Loss: 0.1738, Validation F1: 0.8146\n","Epoch: 16/100, Train Loss: 0.0736, Validation Loss: 0.1749, Validation F1: 0.8150\n","Epoch: 17/100, Train Loss: 0.0722, Validation Loss: 0.1754, Validation F1: 0.8154\n","Epoch: 18/100, Train Loss: 0.0719, Validation Loss: 0.1743, Validation F1: 0.8179\n","Epoch: 19/100, Train Loss: 0.0709, Validation Loss: 0.1752, Validation F1: 0.8157\n","Epoch 00019: reducing learning rate of group 0 to 1.2500e-04.\n","Epoch: 20/100, Train Loss: 0.0694, Validation Loss: 0.1758, Validation F1: 0.8149\n","Epoch: 21/100, Train Loss: 0.0687, Validation Loss: 0.1751, Validation F1: 0.8152\n","Epoch: 22/100, Train Loss: 0.0684, Validation Loss: 0.1757, Validation F1: 0.8168\n","Epoch: 23/100, Train Loss: 0.0681, Validation Loss: 0.1757, Validation F1: 0.8159\n","Epoch: 24/100, Train Loss: 0.0675, Validation Loss: 0.1755, Validation F1: 0.8170\n","Epoch: 25/100, Train Loss: 0.0671, Validation Loss: 0.1763, Validation F1: 0.8135\n","Epoch 00025: reducing learning rate of group 0 to 6.2500e-05.\n","Epoch: 26/100, Train Loss: 0.0668, Validation Loss: 0.1758, Validation F1: 0.8151\n","Epoch: 27/100, Train Loss: 0.0668, Validation Loss: 0.1759, Validation F1: 0.8144\n","Epoch: 28/100, Train Loss: 0.0661, Validation Loss: 0.1754, Validation F1: 0.8152\n","Early stopping due to no improvement in validation f1.\n","run time is : 794.75 seconds\n"]}]},{"cell_type":"code","source":["# print(predictions)"],"metadata":{"id":"Z4iITlfwEBQN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Get the indices of the maximum scores\n","# _, predicted_indices = torch.max(predictions, dim=1)\n","\n","# Step 2: Convert these indices to tag strings\n","# predicted_tags = [idx2tag[idx.item()] for idx in predicted_indices]\n","\n","# print(predicted_tags)"],"metadata":{"id":"7r2OI59se-o-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import precision_recall_fscore_support\n","def evaluate(model, X_data, Y_data):\n","    # Set model to evaluation mode\n","    model.eval()\n","\n","    # Predict tags\n","    with torch.no_grad():\n","        predictions = model(torch.tensor(X_data, dtype=torch.long))\n","        _, predicted_tags = torch.max(predictions, dim=1)\n","\n","    # Flatten both predicted and true tags for evaluation\n","    predicted_tags = predicted_tags.view(-1).numpy()\n","    true_tags = torch.tensor(Y_data, dtype=torch.long).view(-1).numpy()\n","\n","    # Filter out padding tokens (if you have used padding)\n","    non_padding_indices = np.where(true_tags != tag2idx[\"O\"])[0]  # Assuming \"O\" is the padding tag\n","\n","    predicted_tags = predicted_tags[non_padding_indices]\n","    true_tags = true_tags[non_padding_indices]\n","\n","    precision, recall, f1, _ = precision_recall_fscore_support(true_tags, predicted_tags, average='macro')\n","\n","    print(f\"overall f1 score is : {f1:.2f}\")\n","    # Print classification report\n","    print(classification_report(true_tags, predicted_tags, target_names=tags))\n"],"metadata":{"id":"AktG5X3pfGzm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["saved_state_dict = torch.load('best_model.pt')\n","model.load_state_dict(saved_state_dict)\n","evaluate(model, X_dev, Y_dev)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T2t1D_dPfMpC","executionInfo":{"status":"ok","timestamp":1698831494714,"user_tz":-480,"elapsed":1611,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"05edd2d9-78d9-4769-82e0-f5057e94bb84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["overall f1 score is : 0.63\n","              precision    recall  f1-score   support\n","\n","           O       0.00      0.00      0.00         0\n","       I-PER       0.96      0.66      0.78      3149\n","      I-MISC       0.92      0.71      0.80      1268\n","       I-LOC       0.88      0.77      0.82      2094\n","       I-ORG       0.84      0.66      0.74      2092\n","\n","    accuracy                           0.69      8603\n","   macro avg       0.72      0.56      0.63      8603\n","weighted avg       0.91      0.69      0.78      8603\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["evaluate(model, X_test, Y_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"izBgjgq62GdF","executionInfo":{"status":"ok","timestamp":1698831497295,"user_tz":-480,"elapsed":1795,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"d11818ef-3a74-454e-cbce-72e859b70412"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["overall f1 score is : 0.57\n","              precision    recall  f1-score   support\n","\n","           O       0.00      0.00      0.00         0\n","       I-PER       0.96      0.44      0.60      2773\n","      I-MISC       0.85      0.68      0.76       918\n","       I-LOC       0.83      0.74      0.78      1925\n","       I-ORG       0.85      0.58      0.69      2496\n","\n","    accuracy                           0.58      8112\n","   macro avg       0.70      0.49      0.57      8112\n","weighted avg       0.88      0.58      0.69      8112\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]},{"cell_type":"code","source":["evaluate(model, X_train, Y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"By59kP77LTF8","executionInfo":{"status":"ok","timestamp":1698831503452,"user_tz":-480,"elapsed":6159,"user":{"displayName":"Han Hua Teo","userId":"16484726558557177870"}},"outputId":"e5936282-e4b4-499b-f99f-9be8ae81d949"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["overall f1 score is : 0.72\n","              precision    recall  f1-score   support\n","\n","           O       0.00      0.00      0.00         0\n","       I-PER       0.96      0.98      0.97     11128\n","      I-MISC       0.93      0.85      0.89      4593\n","       I-LOC       0.90      0.87      0.88      8297\n","       I-ORG       0.88      0.85      0.86     10025\n","\n","    accuracy                           0.89     34043\n","   macro avg       0.73      0.71      0.72     34043\n","weighted avg       0.92      0.89      0.91     34043\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}